{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Overfitting and Underfitting with Learning Curves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the models overfits, one underfits, and the other one is just right. First, we'll write some code to draw the learning curves for each model, and finally we'll look at the learning curves to decide which model is which.\n",
    "\n",
    "<img src = https://video.udacity-data.com/topher/2017/June/594dbe26_learning-curves/learning-curves.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you like coding, here are some details. We'll be using the function called learning_curve:\n",
    "```python\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator, X, y, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to worry about all the parameters of this function (you can read some more in here, but here we'll explain the main ones:\n",
    "\n",
    "    estimator, is the actual classifier we're using for the data, e.g., LogisticRegression() or GradientBoostingClassifier().\n",
    "    X and y is our data, split into features and labels.\n",
    "    train_sizes are the sizes of the chunks of data used to draw each point in the curve.\n",
    "    train_scores are the training scores for the algorithm trained on each chunk of data.\n",
    "    test_scores are the testing scores for the algorithm trained on each chunk of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two very important observations:\n",
    "\n",
    "- The training and testing scores come in as a list of 3 values, and this is because the function uses 3-Fold Cross-Validation.\n",
    "- very important: As you can see, we defined our curves with Training and Testing Error, and this function defines them with Training and Testing Score. These are opposite, so the higher the error, the lower the score. Thus, when you see the curve, you need to flip it upside down in your mind, in order to compare it with the curves above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search in sklearn is very simple. We'll illustrate it with an example. Let's say we'd like to train a support vector machine, and we'd like to decide between the following parameters:\n",
    "\n",
    "    kernel: poly or rbf.\n",
    "    C: 0.1, 1, or 10.\n",
    "\n",
    "(Note: These parameters can be used as a black box now, but we'll see them in detail in the Supervised Learning Section of the nanodegree.)\n",
    "\n",
    "The steps are the following:\n",
    "1. Import GridSearchCV\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "2. Select the parameters:\n",
    "\n",
    "Here we pick what are the parameters we want to choose from, and form a dictionary. In this dictionary, the keys will be the names of the parameters, and the values will be the lists of possible values for each parameter.\n",
    "```python\n",
    "parameters = {'kernel':['poly', 'rbf'],'C':[0.1, 1, 10]}\n",
    "```\n",
    "3. Create a scorer.\n",
    "\n",
    "We need to decide what metric we'll use to score each of the candidate models. In here, we'll use F1 Score.\n",
    "```python\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "scorer = make_scorer(f1_score)\n",
    "```\n",
    "4. Create a GridSearch Object with the parameters, and the scorer. Use this object to fit the data.\n",
    "```python\n",
    "# Create the object.\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer)\n",
    "# Fit the data\n",
    "grid_fit = grid_obj.fit(X, y)\n",
    "```\n",
    "5. Get the best estimator.\n",
    "```python\n",
    "best_clf = grid_fit.best_estimator_\n",
    "```\n",
    "Now you can use this estimator best_clf to make the predictions.\n",
    "\n",
    "In the next page, you'll find a lab where you can use GridSearchCV to optimize a decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
